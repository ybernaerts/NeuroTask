{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cef79b6e-b5dc-44f5-ab9a-925c126bcc98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from api_neurotask import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plot_utils import adjust_spines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6dd4fa-57a1-40e8-a3aa-78a84e65cdda",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af9f510f-8eef-4ff3-bdfd-09487d7e0868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_names = []\n",
    "for root, dirs, files in os.walk('./data'):\n",
    "    for file in files:\n",
    "        if file.endswith('.parquet'):\n",
    "            dataset_names.append(root + '/' + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38df4d3-05f4-4de4-a439-14c1e1ed5967",
   "metadata": {},
   "source": [
    "Choose dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e67d1fc-a2c6-4ba7-a682-b4720e1b5200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/4_1_MaXuan_Key.parquet'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=dataset_names[0]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed90368-abd5-4c09-bba5-1c9ebfa97d20",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f31009af-57bc-4ace-8182-3b49117bcd24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from ./data/4_1_MaXuan_Key.parquet with bin size of 1 ms\n",
      "Events columns: ['EventGo_cue', 'EventMovement_start', 'EventTarget_Onset']\n",
      "Covariates columns: ['force_x', 'force_y', 'target_dir']\n"
     ]
    }
   ],
   "source": [
    "df, bin = load_and_filter_parquet(dataset, ['A', 'I','F'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ca7cc0df-f431-40f0-8d60-cc6d598457fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neurons = [neuron for neuron in df.columns if neuron.startswith('Neuron')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8d30c-713f-4e05-aa62-b01d26875e0f",
   "metadata": {},
   "source": [
    "### Run GPFA on all sessions and animals in this dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a25a013-67ac-487c-8c0e-0cea79457f8b",
   "metadata": {},
   "source": [
    "We run gpfa, based on core implementation in [elephant](https://elephant.readthedocs.io/en/latest/reference/gpfa.html), but without using Neo spike train preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b55c06c8-f328-471f-a86f-116944ef90f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gpfa_utils import dataframe_to_spike_trains\n",
    "from elephant.gpfa import gpfa_core, gpfa_util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c58557-6533-4236-a946-20b66f9046bc",
   "metadata": {},
   "source": [
    "Hyperparameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f558b0d-f42d-4546-80a9-5e85a9a027b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_dimensionality=6\n",
    "min_var_frac=0.01\n",
    "min_var_frac_explanation=\"\"\"fraction of overall data variance for each observed dimension to set as\n",
    "        the private variance floor.  This is used to combat Heywood cases,\n",
    "        where ML parameter learning returns one or more zero private variances.\n",
    "        Default: 0.01\n",
    "        (See Martin & McDonald, Psychometrika, Dec 1975.)\"\"\"\n",
    "\n",
    "tau_init=100.0 # ms # GP timescale initialization in msec\n",
    "eps_init=1.0e-3 # GP noise variance initialization\n",
    "em_tol=1.0e-8 # stopping criterion for EM\n",
    "em_max_iters=500 # max EM iterations\n",
    "freq_ll=5 # every freq_ll steps in EM likelihood is computed\n",
    "verbose=False # feedback or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13f882-5a81-46e1-af15-12884e0d234b",
   "metadata": {},
   "source": [
    "We want bin sizes at least 10 ms big in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9184d688-20e9-4176-8b55-237afddb56a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_bin=10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1dae28-6139-4a6e-8f88-3ed092bad318",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal id:  1\n",
      "---------\n",
      "Session id:  1\n",
      "Initializing parameters using factor analysis...\n",
      "\n",
      "Fitting GPFA model...\n"
     ]
    }
   ],
   "source": [
    "for animal in df['animal'].unique():\n",
    "    print('Animal id: ', animal)\n",
    "    print('---------')\n",
    "    for session in df[df['animal']==animal]['session'].unique():\n",
    "        print('Session id: ', session)\n",
    "        df_gpfa = df[(df['animal']==animal)&(df['session']==session)]\n",
    "        bin_width=bin\n",
    "        \n",
    "        if bin < 10.0:\n",
    "            df_gpfa = rebin(df_gpfa, prev_bin_size = bin, new_bin_size = new_bin)\n",
    "            bin_width = new_bin\n",
    "        \n",
    "        seqs = dataframe_to_spike_trains(df_gpfa, neurons)\n",
    "        \n",
    "        # Check if training data covariance is full rank\n",
    "        y_all = np.hstack(seqs[\"y\"])\n",
    "        y_dim = y_all.shape[0]\n",
    "\n",
    "        if np.linalg.matrix_rank(np.cov(y_all)) < y_dim:\n",
    "            errmesg = (\n",
    "                \"Observation covariance matrix is rank deficient.\\n\"\n",
    "                \"Possible causes: \"\n",
    "                \"repeated units, not enough observations.\"\n",
    "            )\n",
    "            raise ValueError(errmesg)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Number of training trials: {}\".format(len(seqs)))\n",
    "            print(\"Latent space dimensionality: {}\".format(latent_dimensionality))\n",
    "            print(\n",
    "                \"Observation dimensionality: {}\".format(\n",
    "                    has_spikes_bool.sum()\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        \n",
    "        # Fit\n",
    "        params_estimated, fit_info = gpfa_core.fit(\n",
    "            seqs_train=seqs,\n",
    "            x_dim=latent_dimensionality,\n",
    "            bin_width=bin_width,\n",
    "            min_var_frac=min_var_frac,\n",
    "            em_max_iters=em_max_iters,\n",
    "            em_tol=em_tol,\n",
    "            tau_init=tau_init,\n",
    "            eps_init=eps_init,\n",
    "            freq_ll=freq_ll,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Transform\n",
    "        transform_info = dict()\n",
    "        returned_data=['latent_variable', 'VsmGP']\n",
    "        \n",
    "        seqs, ll = gpfa_core.exact_inference_with_ll(\n",
    "            seqs, params_estimated, get_ll=True\n",
    "        )\n",
    "        transform_info[\"log_likelihood\"] = ll\n",
    "        transform_info[\"num_bins\"] = seqs[\"T\"]\n",
    "        \n",
    "        # Orthonormalize columns in C, update latents\n",
    "        Corth, seqs = gpfa_core.orthonormalize(params_estimated, seqs)\n",
    "        \n",
    "        transform_info[\"Corth\"] = Corth\n",
    "        if len(returned_data) == 1:\n",
    "            gpfa_val_result = seqs[returned_data[0]]\n",
    "        gpfa_val_result =  {x: seqs[x] for x in returned_data}\n",
    "        \n",
    "        with open('save_pickles/{}_animal_{}_session_{}_latent_dim_{}.pickle'.format(\n",
    "            dataset.split('/')[-1].split('.')[0],\n",
    "            animal_id,\n",
    "            session_id,\n",
    "            latent_dimensionality\n",
    "        ), 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'params':params_estimated,\n",
    "                'latents':gpfa_val_result\n",
    "            }, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
