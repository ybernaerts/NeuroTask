{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef79b6e-b5dc-44f5-ab9a-925c126bcc98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from api_neurotask import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plot_utils import adjust_spines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6dd4fa-57a1-40e8-a3aa-78a84e65cdda",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9f510f-8eef-4ff3-bdfd-09487d7e0868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_names = []\n",
    "for root, dirs, files in os.walk('./data'):\n",
    "    for file in files:\n",
    "        if file.endswith('.parquet'):\n",
    "            dataset_names.append(root + '/' + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38df4d3-05f4-4de4-a439-14c1e1ed5967",
   "metadata": {},
   "source": [
    "Choose dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e67d1fc-a2c6-4ba7-a682-b4720e1b5200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/1_4_Makin2_RT.parquet'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=dataset_names[0]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed90368-abd5-4c09-bba5-1c9ebfa97d20",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f31009af-57bc-4ace-8182-3b49117bcd24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from ./data/1_4_Makin2_RT.parquet with bin size of 4 ms\n",
      "Events columns: []\n",
      "Covariates columns: ['target_pos_x', 'target_pos_y', 'cursor_pos_x', 'cursor_pos_y', 'finger_pos_z', 'finger_pos_x', 'finger_pos_y', 'cursor_vel_x', 'cursor_vel_y']\n"
     ]
    }
   ],
   "source": [
    "df, bin = load_and_filter_parquet(dataset, ['A', 'I','F'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7760e303-6b2c-442c-b367-8c29ca46c40f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal</th>\n",
       "      <th>session</th>\n",
       "      <th>unique_trials_per_session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   animal  session  unique_trials_per_session\n",
       "0       1       11                       1025\n",
       "1       1       12                        296\n",
       "2       1       13                        346\n",
       "3       1       14                        294\n",
       "4       1       15                        321\n",
       "5       1       16                        317\n",
       "6       1       17                        329\n",
       "7       1       18                        293\n",
       "8       1       19                        295\n",
       "9       1       20                        346"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['animal', 'session'])['trial_id'].nunique().reset_index(name='unique_trials_per_session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca7cc0df-f431-40f0-8d60-cc6d598457fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neurons = [neuron for neuron in df.columns if neuron.startswith('Neuron')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8d30c-713f-4e05-aa62-b01d26875e0f",
   "metadata": {},
   "source": [
    "### Run GPFA on all sessions and animals in this dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a25a013-67ac-487c-8c0e-0cea79457f8b",
   "metadata": {},
   "source": [
    "We run gpfa, based on core implementation in [elephant](https://elephant.readthedocs.io/en/latest/reference/gpfa.html), but without using Neo spike train preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55c06c8-f328-471f-a86f-116944ef90f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gpfa_utils import dataframe_to_spike_trains\n",
    "from elephant.gpfa import gpfa_core, gpfa_util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c58557-6533-4236-a946-20b66f9046bc",
   "metadata": {},
   "source": [
    "Hyperparameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f558b0d-f42d-4546-80a9-5e85a9a027b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latent_dimensionality=6\n",
    "min_var_frac=0.01\n",
    "min_var_frac_explanation=\"\"\"fraction of overall data variance for each observed dimension to set as\n",
    "        the private variance floor.  This is used to combat Heywood cases,\n",
    "        where ML parameter learning returns one or more zero private variances.\n",
    "        Default: 0.01\n",
    "        (See Martin & McDonald, Psychometrika, Dec 1975.)\"\"\"\n",
    "\n",
    "tau_init=100.0 # ms # GP timescale initialization in msec\n",
    "eps_init=1.0e-3 # GP noise variance initialization\n",
    "em_tol=1.0e-8 # stopping criterion for EM\n",
    "em_max_iters=500 # max EM iterations\n",
    "freq_ll=5 # every freq_ll steps in EM likelihood is computed\n",
    "verbose=False # feedback or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13f882-5a81-46e1-af15-12884e0d234b",
   "metadata": {},
   "source": [
    "We want bin sizes at least 10 ms big in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9184d688-20e9-4176-8b55-237afddb56a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_bin=10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea1dae28-6139-4a6e-8f88-3ed092bad318",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal id:  1\n",
      "---------\n",
      "Session id:  11\n",
      "Initializing parameters using factor analysis...\n",
      "\n",
      "Fitting GPFA model...\n",
      "Session id:  12\n",
      "Initializing parameters using factor analysis...\n",
      "\n",
      "Fitting GPFA model...\n",
      "Session id:  13\n",
      "Initializing parameters using factor analysis...\n",
      "\n",
      "Fitting GPFA model...\n",
      "Session id:  14\n",
      "Initializing parameters using factor analysis...\n",
      "\n",
      "Fitting GPFA model...\n",
      "Session id:  15\n",
      "Initializing parameters using factor analysis...\n",
      "\n",
      "Fitting GPFA model...\n",
      "Session id:  16\n",
      "Initializing parameters using factor analysis...\n",
      "\n",
      "Fitting GPFA model...\n",
      "Session id:  17\n",
      "Initializing parameters using factor analysis...\n",
      "\n",
      "Fitting GPFA model...\n",
      "Session id:  18\n",
      "Initializing parameters using factor analysis...\n",
      "\n",
      "Fitting GPFA model...\n",
      "Session id:  19\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Observation covariance matrix is rank deficient.\nPossible causes: repeated units, not enough observations.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 25\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mmatrix_rank(np\u001b[38;5;241m.\u001b[39mcov(y_all)) \u001b[38;5;241m<\u001b[39m y_dim:\n\u001b[0;32m     20\u001b[0m     errmesg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation covariance matrix is rank deficient.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPossible causes: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepeated units, not enough observations.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m     )\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(errmesg)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of training trials: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(seqs)))\n",
      "\u001b[1;31mValueError\u001b[0m: Observation covariance matrix is rank deficient.\nPossible causes: repeated units, not enough observations."
     ]
    }
   ],
   "source": [
    "for animal in df['animal'].unique():\n",
    "    print('Animal id: ', animal)\n",
    "    print('---------')\n",
    "    for session in df[df['animal']==animal]['session'].unique():\n",
    "        print('Session id: ', session)\n",
    "        df_gpfa = df[(df['animal']==animal)&(df['session']==session)]\n",
    "        bin_width=bin\n",
    "        \n",
    "        if bin < 10.0:\n",
    "            df_gpfa = rebin(df_gpfa, prev_bin_size = bin, new_bin_size = new_bin)\n",
    "            bin_width = new_bin\n",
    "        \n",
    "        seqs = dataframe_to_spike_trains(df_gpfa, neurons)\n",
    "        \n",
    "        # Check if training data covariance is full rank\n",
    "        y_all = np.hstack(seqs[\"y\"])\n",
    "        y_dim = y_all.shape[0]\n",
    "\n",
    "        if np.linalg.matrix_rank(np.cov(y_all)) < y_dim:\n",
    "            print('Observation covariance matrix is rank deficient.')\n",
    "            print('Maybe repeated units, not enough observations.')\n",
    "            print('Skipping this session.')\n",
    "            continue\n",
    "        \n",
    "        #if verbose:\n",
    "        #    print(\"Number of training trials: {}\".format(len(seqs)))\n",
    "        #    print(\"Latent space dimensionality: {}\".format(latent_dimensionality))\n",
    "        #    print(\n",
    "        #        \"Observation dimensionality: {}\".format(\n",
    "        #            has_spikes_bool.sum()\n",
    "        #        )\n",
    "        #    )\n",
    "        \n",
    "        \n",
    "        # Fit\n",
    "        params_estimated, fit_info = gpfa_core.fit(\n",
    "            seqs_train=seqs,\n",
    "            x_dim=latent_dimensionality,\n",
    "            bin_width=bin_width,\n",
    "            min_var_frac=min_var_frac,\n",
    "            em_max_iters=em_max_iters,\n",
    "            em_tol=em_tol,\n",
    "            tau_init=tau_init,\n",
    "            eps_init=eps_init,\n",
    "            freq_ll=freq_ll,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Transform\n",
    "        transform_info = dict()\n",
    "        returned_data=['latent_variable', 'VsmGP']\n",
    "        \n",
    "        seqs, ll = gpfa_core.exact_inference_with_ll(\n",
    "            seqs, params_estimated, get_ll=True\n",
    "        )\n",
    "        transform_info[\"log_likelihood\"] = ll\n",
    "        transform_info[\"num_bins\"] = seqs[\"T\"]\n",
    "        \n",
    "        # Orthonormalize columns in C, update latents\n",
    "        Corth, seqs = gpfa_core.orthonormalize(params_estimated, seqs)\n",
    "        \n",
    "        transform_info[\"Corth\"] = Corth\n",
    "        if len(returned_data) == 1:\n",
    "            gpfa_val_result = seqs[returned_data[0]]\n",
    "        gpfa_val_result =  {x: seqs[x] for x in returned_data}\n",
    "        \n",
    "        with open('save_pickles/{}_animal_{}_session_{}_latent_dim_{}.pickle'.format(\n",
    "            dataset.split('/')[-1].split('.')[0],\n",
    "            animal,\n",
    "            session,\n",
    "            latent_dimensionality\n",
    "        ), 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'params':params_estimated,\n",
    "                'latents':gpfa_val_result\n",
    "            }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06ff14-8b9b-4c4c-b8b9-2ba941b47623",
   "metadata": {},
   "source": [
    "### Run GPFA on all sessions and animals in all datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "656d6bf0-360f-4dae-91b2-197a5e0d21a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/1_4_Makin2_RT.parquet',\n",
       " './data/1_4_Makin3_RT.parquet',\n",
       " './data/1_4_Makin4_RT.parquet',\n",
       " './data/1_4_Makin5_RT.parquet',\n",
       " './data/2_10_Chowdhury_CObump.parquet',\n",
       " './data/2_10_Chowdhury_TRT.parquet',\n",
       " './data/3_30_GallegoCarracedo_CO.parquet',\n",
       " './data/4_1_MaXuan_CO.parquet',\n",
       " './data/4_1_MaXuan_ISO.parquet',\n",
       " './data/4_1_MaXuan_Key.parquet',\n",
       " './data/5_1_Dyer_CO.parquet',\n",
       " './data/6_1_Churchland1_Maze.parquet',\n",
       " './data/6_1_Churchland2_Maze.parquet',\n",
       " './data/6_1_Churchland3_Maze.parquet',\n",
       " './data/6_1_Churchland4_Maze.parquet',\n",
       " './data/6_1_Churchland5_Maze.parquet',\n",
       " './data/6_1_Churchland6_Maze.parquet',\n",
       " './data/6_1_Churchland7_Maze.parquet',\n",
       " './data/6_1_Churchland8_Maze.parquet',\n",
       " './data/6_1_Churchland9_Maze.parquet']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73bab6a7-8ce0-4495-9827-ad81bfac3224",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Data loaded from ./data/3_30_GallegoCarracedo_CO.parquet with bin size of 3e+01 ms\n",
      "Events columns: ['EventGo_cue', 'EventMovement_start', 'EventTarget_Onset']\n",
      "Covariates columns: ['hand_vel_x', 'hand_vel_y', 'target_dir']\n",
      "----------------------------------------------------------------\n",
      "Animal id:  1\n",
      "---------\n",
      "\n",
      "Session id:  1\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m y_all \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack(seqs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     22\u001b[0m y_dim \u001b[38;5;241m=\u001b[39m y_all\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mmatrix_rank(np\u001b[38;5;241m.\u001b[39mcov(y_all)) \u001b[38;5;241m<\u001b[39m y_dim:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObservation covariance matrix is rank deficient.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaybe repeated units, not enough observations.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmatrix_rank\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\linalg\\linalg.py:1883\u001b[0m, in \u001b[0;36mmatrix_rank\u001b[1;34m(A, tol, hermitian)\u001b[0m\n\u001b[0;32m   1881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1882\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(A\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m-> 1883\u001b[0m S \u001b[38;5;241m=\u001b[39m svd(A, compute_uv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, hermitian\u001b[38;5;241m=\u001b[39mhermitian)\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1885\u001b[0m     tol \u001b[38;5;241m=\u001b[39m S\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]) \u001b[38;5;241m*\u001b[39m finfo(S\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39meps\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msvd\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\linalg\\linalg.py:1654\u001b[0m, in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[0;32m   1651\u001b[0m     gufunc \u001b[38;5;241m=\u001b[39m _umath_linalg\u001b[38;5;241m.\u001b[39msvd_n\n\u001b[0;32m   1653\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->d\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1654\u001b[0m s \u001b[38;5;241m=\u001b[39m gufunc(a, signature\u001b[38;5;241m=\u001b[39msignature, extobj\u001b[38;5;241m=\u001b[39mextobj)\n\u001b[0;32m   1655\u001b[0m s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mastype(_realType(result_t), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\linalg\\linalg.py:98\u001b[0m, in \u001b[0;36m_raise_linalgerror_svd_nonconvergence\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_svd_nonconvergence\u001b[39m(err, flag):\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: SVD did not converge"
     ]
    }
   ],
   "source": [
    "for dataset in dataset_names[6:10]:\n",
    "    print('----------------------------------------------------------------')\n",
    "    df, bin = load_and_filter_parquet(dataset, ['A', 'I','F'])\n",
    "    print('----------------------------------------------------------------')\n",
    "    neurons = [neuron for neuron in df.columns if neuron.startswith('Neuron')]\n",
    "    for animal in df['animal'].unique():\n",
    "        print('Animal id: ', animal)\n",
    "        print('---------')\n",
    "        for session in df[df['animal']==animal]['session'].unique():\n",
    "            print('\\nSession id: ', session)\n",
    "            df_gpfa = df[(df['animal']==animal)&(df['session']==session)]\n",
    "            bin_width=bin\n",
    "\n",
    "            if bin < 10.0:\n",
    "                df_gpfa = rebin(df_gpfa, prev_bin_size = bin, new_bin_size = new_bin)\n",
    "                bin_width = new_bin\n",
    "\n",
    "            seqs = dataframe_to_spike_trains(df_gpfa, neurons)\n",
    "\n",
    "            # Check if training data covariance is full rank\n",
    "            y_all = np.hstack(seqs[\"y\"])\n",
    "            y_dim = y_all.shape[0]\n",
    "\n",
    "            if np.linalg.matrix_rank(np.cov(y_all)) < y_dim:\n",
    "                print('Observation covariance matrix is rank deficient.')\n",
    "                print('Maybe repeated units, not enough observations.')\n",
    "                print('Skipping this session.')\n",
    "                continue\n",
    "\n",
    "            #if verbose:\n",
    "            #    print(\"Number of training trials: {}\".format(len(seqs)))\n",
    "            #    print(\"Latent space dimensionality: {}\".format(latent_dimensionality))\n",
    "            #    print(\n",
    "            #        \"Observation dimensionality: {}\".format(\n",
    "            #            has_spikes_bool.sum()\n",
    "            #        )\n",
    "            #    )\n",
    "\n",
    "\n",
    "            # Fit\n",
    "            params_estimated, fit_info = gpfa_core.fit(\n",
    "                seqs_train=seqs,\n",
    "                x_dim=latent_dimensionality,\n",
    "                bin_width=bin_width,\n",
    "                min_var_frac=min_var_frac,\n",
    "                em_max_iters=em_max_iters,\n",
    "                em_tol=em_tol,\n",
    "                tau_init=tau_init,\n",
    "                eps_init=eps_init,\n",
    "                freq_ll=freq_ll,\n",
    "                verbose=verbose\n",
    "            )\n",
    "\n",
    "            # Transform\n",
    "            transform_info = dict()\n",
    "            returned_data=['latent_variable', 'VsmGP']\n",
    "\n",
    "            seqs, ll = gpfa_core.exact_inference_with_ll(\n",
    "                seqs, params_estimated, get_ll=True\n",
    "            )\n",
    "            transform_info[\"log_likelihood\"] = ll\n",
    "            transform_info[\"num_bins\"] = seqs[\"T\"]\n",
    "\n",
    "            # Orthonormalize columns in C, update latents\n",
    "            Corth, seqs = gpfa_core.orthonormalize(params_estimated, seqs)\n",
    "\n",
    "            transform_info[\"Corth\"] = Corth\n",
    "            if len(returned_data) == 1:\n",
    "                gpfa_val_result = seqs[returned_data[0]]\n",
    "            gpfa_val_result =  {x: seqs[x] for x in returned_data}\n",
    "\n",
    "            with open('save_pickles/{}_animal_{}_session_{}_latent_dim_{}.pickle'.format(\n",
    "                dataset.split('/')[-1].split('.')[0],\n",
    "                animal,\n",
    "                session,\n",
    "                latent_dimensionality\n",
    "            ), 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'params':params_estimated,\n",
    "                    'latents':gpfa_val_result\n",
    "                }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a3306-6b9b-4c1e-8d29-a2371682a07b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
